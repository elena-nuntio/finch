{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transformer.png\" width=\"300\">\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunch import Bunch\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch({\n",
    "    'source_max_len': 10,\n",
    "    'target_max_len': 20,\n",
    "    'min_freq': 50,\n",
    "    'hidden_units': 128,\n",
    "    'num_blocks': 2,\n",
    "    'num_heads': 8,\n",
    "    'num_heads': 8,\n",
    "    'dropout_rate': 0.1,\n",
    "    'batch_size': 64,\n",
    "    'position_encoding': 'param',\n",
    "    'activation': 'relu',\n",
    "    'tied_proj_weight': True,\n",
    "    'tied_embedding': True,\n",
    "    'label_smoothing': False,\n",
    "    'lr_decay_strategy': 'exp',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, source_path, target_path):\n",
    "        self.source_words = self.read_data(source_path)\n",
    "        self.target_words = self.read_data(target_path)\n",
    "\n",
    "        self.source_word2idx = self.build_index(self.source_words)\n",
    "        self.target_word2idx = self.build_index(self.target_words, is_target=True)\n",
    "\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "\n",
    "    def build_index(self, data, is_target=False):\n",
    "        chars = [char for line in data.split('\\n') for char in line]\n",
    "        chars = [char for char, freq in Counter(chars).items() if freq > args.min_freq]\n",
    "        if is_target:\n",
    "            symbols = ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "        else:\n",
    "            symbols = ['<pad>','<unk>'] if not args.tied_embedding else ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "\n",
    "\n",
    "    def pad(self, data, word2idx, max_len, is_target=False):\n",
    "        res = []\n",
    "        for line in data.split('\\n'):\n",
    "            temp_line = [word2idx.get(char, word2idx['<unk>']) for char in line]\n",
    "            if len(temp_line) >= max_len:\n",
    "                if is_target:\n",
    "                    temp_line = temp_line[:(max_len-1)] + [word2idx['<end>']]\n",
    "                else:\n",
    "                    temp_line = temp_line[:max_len]\n",
    "            if len(temp_line) < max_len:\n",
    "                if is_target:\n",
    "                    temp_line += ([word2idx['<end>']] + [word2idx['<pad>']]*(max_len-len(temp_line)-1)) \n",
    "                else:\n",
    "                    temp_line += [word2idx['<pad>']] * (max_len - len(temp_line))\n",
    "            res.append(temp_line)\n",
    "        return np.array(res)\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        source_idx = self.pad(self.source_words, self.source_word2idx, args.source_max_len)\n",
    "        target_idx = self.pad(self.target_words, self.target_word2idx, args.target_max_len, is_target=True)\n",
    "        return source_idx, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    \n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def embed_seq(inputs, vocab_size=None, embed_dim=None, zero_pad=False, scale=False):\n",
    "    lookup_table = tf.get_variable('lookup_table', dtype=tf.float32, shape=[vocab_size, embed_dim])\n",
    "    if zero_pad:\n",
    "        lookup_table = tf.concat((tf.zeros([1, embed_dim]), lookup_table[1:, :]), axis=0)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "    if scale:\n",
    "        outputs = outputs * np.sqrt(embed_dim)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def multihead_attn(queries, keys, q_masks, k_masks, num_units=None, num_heads=8,\n",
    "        dropout_rate=args.dropout_rate, future_binding=False, reuse=False, activation=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q]\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k]\n",
    "    \"\"\"\n",
    "    if num_units is None:\n",
    "        num_units = queries.get_shape().as_list[-1]\n",
    "    T_q = queries.get_shape().as_list()[1]                                         # max time length of query\n",
    "    T_k = keys.get_shape().as_list()[1]                                            # max time length of key\n",
    "\n",
    "    Q = tf.layers.dense(queries, num_units, activation, reuse=reuse, name='Q')     # (N, T_q, C)\n",
    "    K_V = tf.layers.dense(keys, 2*num_units, activation, reuse=reuse, name='K_V')    \n",
    "    K, V = tf.split(K_V, 2, -1)        \n",
    "\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)                         # (h*N, T_q, C/h) \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h) \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h)\n",
    "\n",
    "    # Scaled Dot-Product\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0,2,1]))                               # (h*N, T_q, T_k)\n",
    "    align = align / np.sqrt(K_.get_shape().as_list()[-1])                          # scale\n",
    "\n",
    "    # Key Masking\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))                             # exp(-large) -> 0\n",
    "\n",
    "    key_masks = k_masks                                                            # (N, T_k)\n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1])                                 # (h*N, T_k)\n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, T_q, 1])                 # (h*N, T_q, T_k)\n",
    "    align = tf.where(tf.equal(key_masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "\n",
    "    if future_binding:\n",
    "        lower_tri = tf.ones([T_q, T_k])                                            # (T_q, T_k)\n",
    "        lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0], 1, 1])   # (h*N, T_q, T_k)\n",
    "        align = tf.where(tf.equal(masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "    \n",
    "    # Softmax\n",
    "    align = tf.nn.softmax(align)                                                   # (h*N, T_q, T_k)\n",
    "\n",
    "    # Query Masking\n",
    "    query_masks = tf.to_float(q_masks)                                             # (N, T_q)\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1])                             # (h*N, T_q)\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, T_k])            # (h*N, T_q, T_k)\n",
    "    align *= query_masks                                                           # (h*N, T_q, T_k)\n",
    "\n",
    "    align = tf.layers.dropout(align, dropout_rate, training=(not reuse))           # (h*N, T_q, T_k)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(align, V_)                                                 # (h*N, T_q, C/h)\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)              # (N, T_q, C)\n",
    "    # Residual connection\n",
    "    outputs += queries                                                             # (N, T_q, C)   \n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)                                                  # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def pointwise_feedforward(inputs, num_units=[None, None], activation=None):\n",
    "    # Inner layer\n",
    "    outputs = tf.layers.conv1d(inputs, num_units[0], kernel_size=1, activation=activation)\n",
    "    # Readout layer\n",
    "    outputs = tf.layers.conv1d(outputs, num_units[1], kernel_size=1, activation=None)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def learned_position_encoding(inputs, mask, embed_dim):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    outputs = tf.range(tf.shape(inputs)[1])                # (T_q)\n",
    "    outputs = tf.expand_dims(outputs, 0)                   # (1, T_q)\n",
    "    outputs = tf.tile(outputs, [tf.shape(inputs)[0], 1])   # (N, T_q)\n",
    "    outputs = embed_seq(outputs, T, embed_dim, zero_pad=False, scale=False)\n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, num_units):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    position_idx = tf.tile(tf.expand_dims(tf.range(T), 0), [tf.shape(inputs)[0], 1])\n",
    "\n",
    "    position_enc = np.array(\n",
    "        [[pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T)])\n",
    "    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    lookup_table = tf.convert_to_tensor(position_enc, tf.float32)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, position_idx)\n",
    "    \n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    C = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1 - epsilon) * inputs) + (epsilon / C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sources, targets, params, reuse=False):\n",
    "    with tf.variable_scope('forward_pass', reuse=reuse):\n",
    "        pos_enc = _get_position_encoder()\n",
    "\n",
    "        # ENCODER\n",
    "        en_masks = tf.sign(sources)   \n",
    "\n",
    "        with tf.variable_scope('encoder_embedding', reuse=reuse):\n",
    "            encoded = embed_seq(\n",
    "                sources, params['source_vocab_size'], args.hidden_units, zero_pad=True, scale=True)\n",
    "        \n",
    "        with tf.variable_scope('encoder_position_encoding', reuse=reuse):\n",
    "            encoded += pos_enc(sources, en_masks, args.hidden_units)\n",
    "        \n",
    "        with tf.variable_scope('encoder_dropout', reuse=reuse):\n",
    "            encoded = tf.layers.dropout(encoded, args.dropout_rate, training=(not reuse))\n",
    "\n",
    "        for i in range(args.num_blocks):\n",
    "            with tf.variable_scope('encoder_attn_%d'%i, reuse=reuse):\n",
    "                encoded = multihead_attn(queries=encoded, keys=encoded, q_masks=en_masks, k_masks=en_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=False, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('encoder_feedforward_%d'%i, reuse=reuse):\n",
    "                encoded = pointwise_feedforward(encoded, num_units=[4*args.hidden_units, args.hidden_units],\n",
    "                    activation=params['activation'])\n",
    "\n",
    "        # DECODER\n",
    "        decoder_inputs = _shift_right(targets, params['start_symbol'])\n",
    "        de_masks = tf.sign(decoder_inputs)\n",
    "            \n",
    "        if args.tied_embedding:\n",
    "            with tf.variable_scope('encoder_embedding', reuse=True):\n",
    "                decoded = embed_seq(decoder_inputs, params['target_vocab_size'], args.hidden_units,\n",
    "                    zero_pad=True, scale=True)\n",
    "        else:\n",
    "            with tf.variable_scope('decoder_embedding', reuse=reuse):\n",
    "                decoded = embed_seq(\n",
    "                    decoder_inputs, params['target_vocab_size'], args.hidden_units, zero_pad=True, scale=True)\n",
    "        \n",
    "        with tf.variable_scope('decoder_position_encoding', reuse=reuse):\n",
    "            decoded += pos_enc(decoder_inputs, de_masks, args.hidden_units)\n",
    "                \n",
    "        with tf.variable_scope('decoder_dropout', reuse=reuse):\n",
    "            decoded = tf.layers.dropout(decoded, args.dropout_rate, training=(not reuse))\n",
    "\n",
    "        for i in range(args.num_blocks):\n",
    "            with tf.variable_scope('decoder_self_attn_%d'%i, reuse=reuse):\n",
    "                decoded = multihead_attn(queries=decoded, keys=decoded, q_masks=de_masks, k_masks=de_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=True, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('decoder_attn_%d'%i, reuse=reuse):\n",
    "                decoded = multihead_attn(queries=decoded, keys=encoded, q_masks=de_masks, k_masks=en_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=False, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('decoder_feedforward_%d'%i, reuse=reuse):\n",
    "                decoded = pointwise_feedforward(decoded, num_units=[4*args.hidden_units, args.hidden_units],\n",
    "                    activation=params['activation'])\n",
    "        \n",
    "        # OUTPUT LAYER    \n",
    "        if args.tied_proj_weight:\n",
    "            b = tf.get_variable('bias', [params['target_vocab_size']], tf.float32)\n",
    "            _scope = 'encoder_embedding' if args.tied_embedding else 'decoder_embedding'\n",
    "            with tf.variable_scope(_scope, reuse=True):\n",
    "                shared_w = tf.get_variable('lookup_table')\n",
    "            decoded = tf.reshape(decoded, [-1, args.hidden_units])\n",
    "            logits = tf.nn.xw_plus_b(decoded, tf.transpose(shared_w), b)\n",
    "            logits = tf.reshape(logits, [tf.shape(sources)[0], -1, params['target_vocab_size']])\n",
    "        else:\n",
    "            with tf.variable_scope('output_layer', reuse=reuse):\n",
    "                logits = tf.layers.dense(decoded, params['target_vocab_size'], reuse=reuse)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def _model_fn_train(features, mode, params, logits):\n",
    "    with tf.name_scope('backward'):\n",
    "        targets = features['target']\n",
    "        masks = tf.to_float(tf.not_equal(targets, 0))\n",
    "\n",
    "        if args.label_smoothing:\n",
    "            loss_op = label_smoothing_sequence_loss(\n",
    "                logits=logits, targets=targets, weights=masks, label_depth=params['target_vocab_size'])\n",
    "        else:\n",
    "            loss_op = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=logits, targets=targets, weights=masks)\n",
    "\n",
    "        if args.lr_decay_strategy == 'noam':\n",
    "            step_num = tf.train.get_global_step() + 1   # prevents zero global step\n",
    "            lr = _get_noam_lr(step_num)\n",
    "        elif args.lr_decay_strategy == 'exp':\n",
    "            lr = tf.train.exponential_decay(1e-3, tf.train.get_global_step(), 100000, 0.1)\n",
    "        else:\n",
    "            raise ValueError(\"lr decay strategy must be one of 'noam' and 'exp'\")\n",
    "        log_hook = tf.train.LoggingTensorHook({'lr': lr}, every_n_iter=100)\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss_op, train_op=train_op, training_hooks=[log_hook])\n",
    "\n",
    "\n",
    "def _model_fn_predict(features, mode, params):\n",
    "    def cond(i, x, temp):\n",
    "        return i < args.target_max_len\n",
    "\n",
    "    def body(i, x, temp):\n",
    "        logits = forward_pass(features['source'], x, params, reuse=True)\n",
    "        ids = tf.argmax(logits, -1)[:, i]\n",
    "        ids = tf.expand_dims(ids, -1)\n",
    "\n",
    "        temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "\n",
    "        x = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "        x = tf.reshape(x, [tf.shape(temp)[0], args.target_max_len])\n",
    "        i += 1\n",
    "        return i, x, temp\n",
    "\n",
    "    _, res, _ = tf.while_loop(cond, body, [tf.constant(0), features['target'], features['target']])\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=res)\n",
    "\n",
    "\n",
    "def tf_estimator_model_fn(features, labels, mode, params):\n",
    "    logits = forward_pass(features['source'], features['target'], params)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        _ = forward_pass(features['source'], features['target'], params, reuse=True)\n",
    "        return _model_fn_train(features, mode, params, logits)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return _model_fn_predict(features, mode, params)\n",
    "\n",
    "\n",
    "def _shift_right(targets, start_symbol):\n",
    "    start_symbols = tf.cast(tf.fill([tf.shape(targets)[0], 1], start_symbol), tf.int64)\n",
    "    return tf.concat([start_symbols, targets[:, :-1]], axis=-1)\n",
    "\n",
    "\n",
    "def _get_position_encoder():\n",
    "    if args.position_encoding == 'non_param':\n",
    "        pos_enc = sinusoidal_position_encoding\n",
    "    elif args.position_encoding == 'param':\n",
    "        pos_enc = learned_position_encoding\n",
    "    else:\n",
    "        raise ValueError(\"position encoding has to be either 'param' or 'non_param'\")\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "def _get_noam_lr(step_num):\n",
    "    return tf.rsqrt(tf.to_float(args.hidden_units)) * tf.minimum(\n",
    "        tf.rsqrt(tf.to_float(step_num)),\n",
    "        tf.to_float(step_num) * tf.convert_to_tensor(args.warmup_steps ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"source_max_len\": 10,\n",
      "    \"target_max_len\": 20,\n",
      "    \"min_freq\": 50,\n",
      "    \"hidden_units\": 128,\n",
      "    \"num_blocks\": 2,\n",
      "    \"num_heads\": 8,\n",
      "    \"dropout_rate\": 0.1,\n",
      "    \"batch_size\": 64,\n",
      "    \"position_encoding\": \"param\",\n",
      "    \"activation\": \"relu\",\n",
      "    \"tied_proj_weight\": true,\n",
      "    \"tied_embedding\": true,\n",
      "    \"label_smoothing\": false,\n",
      "    \"lr_decay_strategy\": \"exp\"\n",
      "}\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1134b6c50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.509198, step = 1\n",
      "INFO:tensorflow:lr = 0.001\n",
      "INFO:tensorflow:global_step/sec: 4.97885\n",
      "INFO:tensorflow:loss = 0.27305442, step = 101 (20.086 sec)\n",
      "INFO:tensorflow:lr = 0.0009977 (20.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.17641\n",
      "INFO:tensorflow:loss = 0.27426323, step = 201 (19.319 sec)\n",
      "INFO:tensorflow:lr = 0.0009954055 (19.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.33645\n",
      "INFO:tensorflow:loss = 0.13783169, step = 301 (18.739 sec)\n",
      "INFO:tensorflow:lr = 0.0009931161 (18.739 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.36203\n",
      "INFO:tensorflow:loss = 0.1400608, step = 401 (18.650 sec)\n",
      "INFO:tensorflow:lr = 0.000990832 (18.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.42184\n",
      "INFO:tensorflow:loss = 0.18023604, step = 501 (18.444 sec)\n",
      "INFO:tensorflow:lr = 0.0009885532 (18.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.27943\n",
      "INFO:tensorflow:loss = 0.12116771, step = 601 (18.941 sec)\n",
      "INFO:tensorflow:lr = 0.0009862796 (18.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.37667\n",
      "INFO:tensorflow:loss = 0.12078158, step = 701 (18.599 sec)\n",
      "INFO:tensorflow:lr = 0.0009840112 (18.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.35767\n",
      "INFO:tensorflow:loss = 0.10558135, step = 801 (18.665 sec)\n",
      "INFO:tensorflow:lr = 0.000981748 (18.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.36969\n",
      "INFO:tensorflow:loss = 0.07728226, step = 901 (18.623 sec)\n",
      "INFO:tensorflow:lr = 0.00097949 (18.623 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09151925.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelpp\n",
      "common -> cmmnoo\n",
      "zhedong -> deghnozz\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.080827825, step = 1001\n",
      "INFO:tensorflow:lr = 0.0009772372\n",
      "INFO:tensorflow:global_step/sec: 4.97633\n",
      "INFO:tensorflow:loss = 0.090015665, step = 1101 (20.096 sec)\n",
      "INFO:tensorflow:lr = 0.00097498967 (20.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.41721\n",
      "INFO:tensorflow:loss = 0.11027878, step = 1201 (18.460 sec)\n",
      "INFO:tensorflow:lr = 0.0009727473 (18.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.40334\n",
      "INFO:tensorflow:loss = 0.06404703, step = 1301 (18.507 sec)\n",
      "INFO:tensorflow:lr = 0.00097051 (18.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.4096\n",
      "INFO:tensorflow:loss = 0.083010316, step = 1401 (18.486 sec)\n",
      "INFO:tensorflow:lr = 0.0009682779 (18.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.32957\n",
      "INFO:tensorflow:loss = 0.084350444, step = 1501 (18.763 sec)\n",
      "INFO:tensorflow:lr = 0.0009660509 (18.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.42173\n",
      "INFO:tensorflow:loss = 0.06199441, step = 1601 (18.444 sec)\n",
      "INFO:tensorflow:lr = 0.0009638291 (18.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.41032\n",
      "INFO:tensorflow:loss = 0.055603825, step = 1701 (18.483 sec)\n",
      "INFO:tensorflow:lr = 0.0009616123 (18.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.42812\n",
      "INFO:tensorflow:loss = 0.05987442, step = 1801 (18.423 sec)\n",
      "INFO:tensorflow:lr = 0.0009594007 (18.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.18458\n",
      "INFO:tensorflow:loss = 0.058285914, step = 1901 (19.288 sec)\n",
      "INFO:tensorflow:lr = 0.00095719413 (19.288 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.049808737.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelp\n",
      "common -> cmmno\n",
      "zhedong -> deghnoz\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.12213433, step = 2001\n",
      "INFO:tensorflow:lr = 0.00095499266\n",
      "INFO:tensorflow:global_step/sec: 4.91494\n",
      "INFO:tensorflow:loss = 0.05108366, step = 2101 (20.347 sec)\n",
      "INFO:tensorflow:lr = 0.0009527962 (20.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.40439\n",
      "INFO:tensorflow:loss = 0.05291336, step = 2201 (18.503 sec)\n",
      "INFO:tensorflow:lr = 0.00095060485 (18.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.38238\n",
      "INFO:tensorflow:loss = 0.07041939, step = 2301 (18.580 sec)\n",
      "INFO:tensorflow:lr = 0.00094841846 (18.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.36339\n",
      "INFO:tensorflow:loss = 0.084211364, step = 2401 (18.645 sec)\n",
      "INFO:tensorflow:lr = 0.0009462372 (18.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.38185\n",
      "INFO:tensorflow:loss = 0.052624665, step = 2501 (18.581 sec)\n",
      "INFO:tensorflow:lr = 0.0009440609 (18.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.30109\n",
      "INFO:tensorflow:loss = 0.060030855, step = 2601 (18.864 sec)\n",
      "INFO:tensorflow:lr = 0.00094188965 (18.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.33131\n",
      "INFO:tensorflow:loss = 0.03270989, step = 2701 (18.757 sec)\n",
      "INFO:tensorflow:lr = 0.00093972334 (18.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.24857\n",
      "INFO:tensorflow:loss = 0.08420595, step = 2801 (19.053 sec)\n",
      "INFO:tensorflow:lr = 0.000937562 (19.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.18729\n",
      "INFO:tensorflow:loss = 0.038027443, step = 2901 (19.278 sec)\n",
      "INFO:tensorflow:lr = 0.0009354057 (19.278 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.03721501.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelpp\n",
      "common -> cmmnoo\n",
      "zhedong -> deghnoz\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.02704989, step = 3001\n",
      "INFO:tensorflow:lr = 0.00093325437\n",
      "INFO:tensorflow:global_step/sec: 4.91579\n",
      "INFO:tensorflow:loss = 0.038575374, step = 3101 (20.344 sec)\n",
      "INFO:tensorflow:lr = 0.0009311079 (20.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.24715\n",
      "INFO:tensorflow:loss = 0.047149014, step = 3201 (19.058 sec)\n",
      "INFO:tensorflow:lr = 0.00092896644 (19.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.21059\n",
      "INFO:tensorflow:loss = 0.021981483, step = 3301 (19.192 sec)\n",
      "INFO:tensorflow:lr = 0.0009268299 (19.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.34537\n",
      "INFO:tensorflow:loss = 0.037699077, step = 3401 (18.708 sec)\n",
      "INFO:tensorflow:lr = 0.0009246982 (18.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.21334\n",
      "INFO:tensorflow:loss = 0.0477191, step = 3501 (19.182 sec)\n",
      "INFO:tensorflow:lr = 0.00092257146 (19.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.38716\n",
      "INFO:tensorflow:loss = 0.042922914, step = 3601 (18.563 sec)\n",
      "INFO:tensorflow:lr = 0.0009204496 (18.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.39552\n",
      "INFO:tensorflow:loss = 0.038485523, step = 3701 (18.534 sec)\n",
      "INFO:tensorflow:lr = 0.0009183326 (18.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.41883\n",
      "INFO:tensorflow:loss = 0.03432907, step = 3801 (18.454 sec)\n",
      "INFO:tensorflow:lr = 0.00091622054 (18.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.35961\n",
      "INFO:tensorflow:loss = 0.042991675, step = 3901 (18.658 sec)\n",
      "INFO:tensorflow:lr = 0.00091411325 (18.658 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.033020332.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelpp\n",
      "common -> cmmnoo\n",
      "zhedong -> deghnoz\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4001 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.05327564, step = 4001\n",
      "INFO:tensorflow:lr = 0.0009120109\n",
      "INFO:tensorflow:global_step/sec: 4.91703\n",
      "INFO:tensorflow:loss = 0.02107259, step = 4101 (20.339 sec)\n",
      "INFO:tensorflow:lr = 0.00090991333 (20.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.12749\n",
      "INFO:tensorflow:loss = 0.0045398325, step = 4201 (19.504 sec)\n",
      "INFO:tensorflow:lr = 0.0009078206 (19.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.29206\n",
      "INFO:tensorflow:loss = 0.021206874, step = 4301 (18.895 sec)\n",
      "INFO:tensorflow:lr = 0.0009057327 (18.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.28297\n",
      "INFO:tensorflow:loss = 0.017140405, step = 4401 (18.929 sec)\n",
      "INFO:tensorflow:lr = 0.0009036495 (18.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.19091\n",
      "INFO:tensorflow:loss = 0.056376163, step = 4501 (19.265 sec)\n",
      "INFO:tensorflow:lr = 0.0009015712 (19.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.28341\n",
      "INFO:tensorflow:loss = 0.03347956, step = 4601 (18.927 sec)\n",
      "INFO:tensorflow:lr = 0.0008994976 (18.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.24488\n",
      "INFO:tensorflow:loss = 0.023245271, step = 4701 (19.066 sec)\n",
      "INFO:tensorflow:lr = 0.00089742884 (19.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.03954\n",
      "INFO:tensorflow:loss = 0.040366407, step = 4801 (19.843 sec)\n",
      "INFO:tensorflow:lr = 0.0008953648 (19.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.0041\n",
      "INFO:tensorflow:loss = 0.011146572, step = 4901 (19.984 sec)\n",
      "INFO:tensorflow:lr = 0.0008933055 (19.983 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.02142885.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpes31u1s7/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelpp\n",
      "common -> cmmnoo\n",
      "zhedong -> deghnoz\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(test_words, tf_estimator, dl):\n",
    "    test_indices = []\n",
    "    for test_word in test_words:\n",
    "        test_idx = [dl.source_word2idx[c] for c in test_word] + \\\n",
    "                   [dl.source_word2idx['<pad>']] * (args.source_max_len - len(test_word))\n",
    "        test_indices.append(test_idx)\n",
    "    test_indices = np.atleast_2d(test_indices)\n",
    "    \n",
    "    zeros = np.zeros([len(test_words), args.target_max_len], np.int64)\n",
    "\n",
    "    pred_ids = tf_estimator.predict(tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'source':test_indices, 'target':zeros}, batch_size=len(test_words), shuffle=False))\n",
    "    pred_ids = list(pred_ids)\n",
    "    \n",
    "    target_idx2word = {i: w for w, i in dl.target_word2idx.items()}\n",
    "    for i, test_word in enumerate(test_words):\n",
    "        ans = ''.join([target_idx2word[id] for id in pred_ids[i]])\n",
    "        print(test_word, '->', ans.replace('<end>', ''))\n",
    "\n",
    "\n",
    "def prepare_params(dl):\n",
    "    if args.activation == 'relu':\n",
    "        activation = tf.nn.relu\n",
    "    elif args.activation == 'elu':\n",
    "        activation = tf.nn.elu\n",
    "    elif args.activation == 'lrelu':\n",
    "        activation = tf.nn.leaky_relu\n",
    "    else:\n",
    "        raise ValueError(\"acitivation fn has to be 'relu' or 'elu' or 'lrelu'\")\n",
    "    params = {\n",
    "        'source_vocab_size': len(dl.source_word2idx),\n",
    "        'target_vocab_size': len(dl.target_word2idx),\n",
    "        'start_symbol': dl.target_word2idx['<start>'],\n",
    "        'activation': activation}\n",
    "    return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(json.dumps(args, indent=4))\n",
    "    \n",
    "    dl = DataLoader(\n",
    "        source_path='../temp/letters_source.txt',\n",
    "        target_path='../temp/letters_target.txt')\n",
    "    sources, targets = dl.load()\n",
    "    \n",
    "    tf_estimator = tf.estimator.Estimator(\n",
    "        tf_estimator_model_fn, params=prepare_params(dl))\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        tf_estimator.train(tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {'source':sources, 'target':targets},\n",
    "            batch_size = args.batch_size,\n",
    "            num_epochs = None,\n",
    "            shuffle = True), steps=1000)\n",
    "        greedy_decode(['apple', 'common', 'zhedong'], tf_estimator, dl)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
