{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vae.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modified import ModifiedBasicDecoder, ModifiedBeamSearchDecoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'max_len': 15,\n",
    "    'word_dropout_rate': 0.2,\n",
    "    'beam_width': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(index_from=4):\n",
    "    PARAMS['word2idx'] = tf.keras.datasets.imdb.get_word_index()\n",
    "    PARAMS['word2idx'] = {k: (v + index_from) for k, v in PARAMS['word2idx'].items()}\n",
    "    PARAMS['word2idx']['<pad>'] = 0\n",
    "    PARAMS['word2idx']['<start>'] = 1\n",
    "    PARAMS['word2idx']['<unk>'] = 2\n",
    "    PARAMS['word2idx']['<end>'] = 3\n",
    "    \n",
    "    PARAMS['idx2word'] = {i: w for w, i in PARAMS['word2idx'].items()}\n",
    "    PARAMS['idx2word'][-1] = '-1'     # task-specific exception handling\n",
    "    PARAMS['idx2word'][4] = '4'       # task-specific exception handling\n",
    "\n",
    "    \n",
    "def load_data(index_from=4):\n",
    "    (X_train, _), (X_test, _) = tf.contrib.keras.datasets.imdb.load_data(\n",
    "        num_words=None, index_from=index_from)\n",
    "    return (X_train, X_test)\n",
    "\n",
    "\n",
    "def word_dropout(x):\n",
    "    is_dropped = np.random.binomial(1, PARAMS['word_dropout_rate'], x.shape)\n",
    "    fn = np.vectorize(lambda x, k: PARAMS['word2idx']['<unk>'] if (\n",
    "        k and (x not in range(4))) else x)\n",
    "    return fn(x, is_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> shown in australia as 'hydrosphere' this incredibly bad movie is so bad that you\n",
      "\n",
      "shown in australia as 'hydrosphere' this incredibly bad movie is so bad that you\n",
      "\n",
      "<start> shown in <unk> as <unk> this incredibly bad movie is so bad that you\n",
      "\n",
      "shown in australia as 'hydrosphere' this incredibly bad movie is so bad that you <end>\n"
     ]
    }
   ],
   "source": [
    "word2idx = build_vocab()\n",
    "X = np.concatenate(load_data())\n",
    "\n",
    "X = np.concatenate((\n",
    "    tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X, PARAMS['max_len'], truncating='post', padding='post'),\n",
    "    tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X, PARAMS['max_len'], truncating='pre', padding='post')))\n",
    "\n",
    "enc_inp = X[:, 1:]\n",
    "dec_inp = word_dropout(X)\n",
    "dec_out = np.concatenate([X[:, 1:], np.full([X.shape[0], 1],\n",
    "                                            PARAMS['word2idx']['<end>'])], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparam_trick(z_mean, z_logvar):\n",
    "    gaussian = tf.truncated_normal(tf.shape(z_logvar))\n",
    "    z = z_mean + tf.exp(0.5 * z_logvar) * gaussian\n",
    "    return z\n",
    "\n",
    "\n",
    "def kl_w_fn(anneal_max, anneal_bias, global_step):\n",
    "    return anneal_max * tf.sigmoid((10 / anneal_bias) * (\n",
    "        tf.to_float32(global_step) - tf.constant(anneal_bias / 2)))\n",
    "\n",
    "\n",
    "def kl_loss_fn(self, mean, gamma):\n",
    "    return 0.5 * tf.reduce_sum(\n",
    "        tf.exp(gamma) + tf.square(mean) - 1 - gamma) / tf.to_float(tf.shape(mean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell():\n",
    "    return tf.nn.rnn_cell.GRUCell(PARAMS['rnn_size'],\n",
    "                                  kernel_initializer=tf.orthogonal_initializer())\n",
    "\n",
    "\n",
    "def forward(inputs, labels, reuse, is_training):\n",
    "    enc_seq_len = tf.count_nonzero(inputs, 1, dtype=tf.int32)\n",
    "    dec_seq_len = tf.count_nonzero(labels, 1, dtype=tf.int32)\n",
    "    batch_sz = tf.shape(inputs)[0]\n",
    "    \n",
    "    with tf.variable_scope('Encoder', reuse=reuse):\n",
    "        embedding = tf.get_variable('lookup_table', [len(PARAMS['word2idx']), PARAMS['embed_dims']])\n",
    "        x = tf.nn.embedding_lookup(embedding, inputs)\n",
    "        \n",
    "        _, enc_state = tf.nn.dynamic_rnn(rnn_cell(), x, enc_seq_len, dtype=tf.float32)\n",
    "        \n",
    "        z_mean = tf.layers.dense(enc_state, PARAMS['latent_size'])\n",
    "        z_logvar = tf.layers.dense(enc_state, PARAMS['latent_size'])\n",
    "        \n",
    "    z = reparam_trick(z_mean, z_logvar)\n",
    "        \n",
    "    with tf.variable_scope('Decoder', reuse=reuse):\n",
    "        output_proj = tf.layers.Dense(len(PARAMS['word2idx']))\n",
    "        dec_cell = rnn_cell()\n",
    "        \n",
    "        if is_training:\n",
    "            init_state = cell.zero_state(batch_sz, tf.float32).clone(\n",
    "                cell_state=enc_state)\n",
    "            \n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs = tf.nn.embedding_lookup(embedding, labels['dec_inp']),\n",
    "                sequence_length = dec_seq_len)\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = cell,\n",
    "                helper = helper,\n",
    "                initial_state = init_state,\n",
    "                output_layer = output_proj)\n",
    "            decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = decoder,\n",
    "                maximum_iterations = tf.reduce_max(dec_seq_len))\n",
    "            return decoder_output.rnn_output\n",
    "        else:\n",
    "            decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell = cell,\n",
    "                embedding = embedding,\n",
    "                start_tokens = tf.tile(tf.constant([PARAMS['word2idx']['<start>']], tf.int32),\n",
    "                                       [batch_sz]),\n",
    "                end_token = PARAMS['word2idx']['<end>'],\n",
    "                initial_state = init_state,\n",
    "                beam_width = PARAMS['beam_width'],\n",
    "                output_layer = output_proj)\n",
    "            decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = decoder)\n",
    "            return decoder_output.predicted_ids[:, :, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
